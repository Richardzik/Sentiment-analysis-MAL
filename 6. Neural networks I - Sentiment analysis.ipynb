{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bc9be56",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818b3b52",
   "metadata": {},
   "source": [
    "In this exercise we use the IMDb-dataset, which we will use to perform a sentiment analysis. The code below assumes that the data is placed in the same folder as this notebook. We see that the reviews are loaded as a pandas dataframe, and print the beginning of the first few reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67da3bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "                                                   0\n",
      "0  bromwell high is a cartoon comedy . it ran at ...\n",
      "1  story of a man who has unnatural feelings for ...\n",
      "2  homelessness  or houselessness as george carli...\n",
      "3  airport    starts as a brand new luxury    pla...\n",
      "4  brilliant over  acting by lesley ann warren . ...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping # Added EarlyStopping\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "from numpy.random import seed, randint\n",
    "\n",
    "reviews = pd.read_csv('reviews.txt', header=None)\n",
    "labels = pd.read_csv('labels.txt', header=None)\n",
    "Y = (labels=='positive').astype(np.int_)\n",
    "\n",
    "print(type(reviews))\n",
    "print(reviews.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4982b946",
   "metadata": {},
   "source": [
    "**(a)** Split the reviews and labels in test, train and validation sets. The train and validation sets will be used to train your model and tune hyperparameters, the test set will be saved for testing. Use the `CountVectorizer` from `sklearn.feature_extraction.text` to create a Bag-of-Words representation of the reviews. Only use the 10,000 most frequent words (use the `max_features`-parameter of `CountVectorizer`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf07ee9",
   "metadata": {},
   "source": [
    "**(b)** Explore the representation of the reviews. How is a single word represented? How about a whole review?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2638fce",
   "metadata": {},
   "source": [
    "**(c)** Train a neural network with a single hidden layer on the dataset, tuning the relevant hyperparameters to optimize accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd327a6",
   "metadata": {},
   "source": [
    "**(d)** Test your sentiment-classifier on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd44ee62",
   "metadata": {},
   "source": [
    "**(e)** Use the classifier to classify a few sentences you write yourselves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1ef2970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14062, 10000)\n",
      "(4688, 10000)\n",
      "(6250, 10000)\n"
     ]
    }
   ],
   "source": [
    "#A)\n",
    "\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(reviews[0], Y[0] , random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, \n",
    "                                                  random_state=43)\n",
    "\n",
    "count_vectorizer = CountVectorizer(max_features=10000)\n",
    "X_train_bow = count_vectorizer.fit_transform(X_train)\n",
    "X_val_bow = count_vectorizer.transform(X_val)\n",
    "X_test_bow = count_vectorizer.transform(X_test)\n",
    "\n",
    "print(X_train_bow.shape)\n",
    "print(X_val_bow.shape)\n",
    "print(X_test_bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2088c940-51f5-4b84-aab0-6143de9a8d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single words\n",
      "the: 189639\n",
      "and: 92662\n",
      "of: 81947\n",
      "to: 76525\n",
      "is: 60238\n",
      "br: 57951\n",
      "it: 54143\n",
      "in: 52645\n",
      "this: 42800\n",
      "that: 41490\n",
      "was: 27177\n",
      "as: 26199\n",
      "with: 24930\n",
      "for: 24876\n",
      "movie: 24663\n",
      "but: 24014\n",
      "film: 22633\n",
      "on: 19237\n",
      "you: 19123\n",
      "not: 17170\n",
      "he: 16856\n",
      "are: 16513\n",
      "his: 16441\n",
      "have: 15701\n",
      "be: 15166\n",
      "one: 15123\n",
      "all: 13579\n",
      "at: 13266\n",
      "they: 12683\n",
      "by: 12631\n",
      "who: 12078\n",
      "an: 12051\n",
      "from: 11532\n",
      "so: 11485\n",
      "like: 11294\n",
      "there: 10550\n",
      "her: 10346\n",
      "or: 10061\n",
      "just: 10059\n",
      "about: 9690\n",
      "out: 9667\n",
      "if: 9386\n",
      "has: 9364\n",
      "what: 9080\n",
      "some: 8931\n",
      "good: 8460\n",
      "can: 8205\n",
      "more: 8039\n",
      "she: 8001\n",
      "when: 7971\n",
      "\n",
      "\n",
      "Review words\n",
      "the: 4\n",
      "it: 3\n",
      "and: 2\n",
      "wacky: 2\n",
      "see: 2\n",
      "this: 1\n",
      "movie: 1\n",
      "is: 1\n",
      "not: 1\n",
      "only: 1\n",
      "funniest: 1\n",
      "film: 1\n",
      "ever: 1\n",
      "created: 1\n",
      "greatest: 1\n",
      "my: 1\n",
      "hats: 1\n",
      "off: 1\n",
      "to: 1\n",
      "mr: 1\n",
      "mrs: 1\n",
      "rest: 1\n",
      "of: 1\n",
      "cast: 1\n",
      "good: 1\n",
      "morning: 1\n",
      "satan: 1\n",
      "want: 1\n",
      "post: 1\n",
      "go: 1\n",
      "now: 1\n"
     ]
    }
   ],
   "source": [
    "#B) \n",
    "# for a single word\n",
    "print(\"Single words\")\n",
    "word_counts = np.asarray(X_train_bow.sum(axis=0)).flatten()\n",
    "vocab = count_vectorizer.get_feature_names_out()\n",
    "\n",
    "word_freq = list(zip(vocab, word_counts))\n",
    "word_freq_sorted = sorted(word_freq, key=lambda x: x[1], reverse=True)\n",
    "# Print top 50 most common words\n",
    "for word, freq in word_freq_sorted[:50]:\n",
    "    print(f\"{word}: {freq}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# for a review\n",
    "print(\"Review words\")\n",
    "sample_idx = 0\n",
    "sample_vector = X_train_bow[sample_idx]\n",
    "\n",
    "vocab = count_vectorizer.get_feature_names_out()\n",
    "\n",
    "word_indices = sample_vector.nonzero()[1]\n",
    "word_counts = sample_vector.data\n",
    "\n",
    "bow_representation = {vocab[i]: count for i, count in zip(word_indices, word_counts)}\n",
    "\n",
    "for word, count in sorted(bow_representation.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728883bf-39c0-486f-b8fd-13c27051ad07",
   "metadata": {},
   "source": [
    "## Single word representation\n",
    "<b>Single words are represented in a way that, the higher is the word in Bag-of-Words, the more frequently it is used in a review. We can say they are represented in a hierarchy most to least frequently used words</b>\n",
    "\n",
    "## Whole review representation\n",
    "<b>In whole review representation we can see how often each word was used in the context of that, exact review. We can see how repetitive the person was while writing the review</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76e027b5-6897-4ff1-8837-a02b1d38eb4e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_train_oh' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m sgd = SGD(learning_rate = \u001b[32m0.1\u001b[39m)\n\u001b[32m     12\u001b[39m model.compile(loss = \u001b[33m'\u001b[39m\u001b[33mcategorical_crossentropy\u001b[39m\u001b[33m'\u001b[39m, optimizer = sgd, metrics = [\u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m history = model.fit(X_train_bow, \u001b[43my_train_oh\u001b[49m, \n\u001b[32m     15\u001b[39m                     epochs=\u001b[32m10\u001b[39m, batch_size=\u001b[32m50\u001b[39m, verbose=\u001b[32m1\u001b[39m,\n\u001b[32m     16\u001b[39m                     validation_data=(X_val_bow, y_val_oh))\n",
      "\u001b[31mNameError\u001b[39m: name 'y_train_oh' is not defined"
     ]
    }
   ],
   "source": [
    "#C)\n",
    "num_classes = 10\n",
    "input_size = reviews[0].shape[0]\n",
    "seed(0)\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "model = Sequential() # initialize a neural network\n",
    "model.add(Dense(units = 32, activation='tanh', input_dim=input_size)) # add a hidden layer\n",
    "model.add(Dense(units = num_classes, activation='softmax')) #add the output layer\n",
    "\n",
    "sgd = SGD(learning_rate = 0.1)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = sgd, metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit(X_train_bow, y_train_oh, \n",
    "                    epochs=10, batch_size=50, verbose=1,\n",
    "                    validation_data=(X_val_bow, y_val_oh))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
